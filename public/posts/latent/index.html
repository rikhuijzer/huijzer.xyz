<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

   <!-- clipboard.js -->
<script defer src="/libs/clipboard.min.js"></script>

<!-- highlight.js -->
<link rel="stylesheet" href="/libs/highlight/github.min.css" media="(prefers-color-scheme: light)">
<link rel="stylesheet" href="/libs/highlight/github-dark.min.css" media="(prefers-color-scheme: dark)">

<script defer src="/libs/highlight/highlight.min.js"></script>
<script defer src="/libs/highlight/julia.min.js"></script>
<script defer src="/libs/highlight/rust.min.js"></script>

<script defer>
    document.addEventListener('DOMContentLoaded', function() {
        hljs.highlightAll();
    });
</script>
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title> Bayesian Latent Profile Analysis (mixture modeling) - Rik Huijzer </title> 
  

  <meta property="og:title" content="Bayesian Latent Profile Analysis (mixture modeling)" />
  <meta property="og:type" content="article" /> 
  <meta property="og:description" content="This post discusses some latent analysis techniques and runs a Bayesian analysis for example data where the outcome is continuous, also known as latent profile analysis (LPA)." />
  <!-- <meta property="og:image" content="{{website_url}}{{requiredfill image}}" /> -->

  <meta name="twitter:title" content="Bayesian Latent Profile Analysis (mixture modeling)" />
  <meta name="twitter:card" content="summary" />
</head>
<body>
  <header>
<div class="blog-name"><a href="/">HUIJZER.XYZ</a></div>
<nav>
  <ul>
    <li><a href="/posts/">Blog</a></li>
    <li><a href="/about/">About</a></li>
    <li><a type="application/rss+xml" href="https://huijzer.xyz/feed.xml">
      RSS
      </a></li>
  </ul>
</nav>
</header>


<div class="franklin-content">
   <h1 class="page-title"> Bayesian Latent Profile Analysis (mixture modeling) </h1> 
   <span class="page-date"> 2021-12-01 </span> 
</div>
<div class="franklin-content">
<p><em>Updated on 2021-12-15: Include ordered constraint.</em></p>
<p>This post discusses some latent analysis techniques and runs a Bayesian analysis for example data where the outcome is continuous, also known as <em>latent profile analysis</em> &#40;LPA&#41;. My aim will be to clearly visualize the analysis so that it can easily be adjusted to different contexts.</p>
<p>In essence, latent analyses about finding hidden groups in data &#40;Oberski, 2016&#41;. Specifically, they are called <em>mixture models</em> because the underlying distributions are mixed together.</p>
<p>For example, suppose we had data about dog weights and did not register the breed. If we assume that the sample only consists of Australian Sheperds and American Terriers, that American Terriers are larger on average, and that both groups are normally distributed, then we can take the observed data and estimate the latent distributions. For example, we can generate some data:</p>
<pre><code class="language-julia">begin
    using Bijectors: OrderedBijector, ordered, inv
    using CairoMakie
    using DataFrames
    using StableRNGs: StableRNG
    using Statistics: mean, median, std
    using Turing
end</code></pre>
<pre><code class="language-julia">d1 &#61; Normal&#40;22, 2.4&#41;; # Australian Shepherd.</code></pre>
<pre><code class="language-julia">d2 &#61; Normal&#40;28, 2.8&#41;; # American Terrier.</code></pre>
<pre><code class="language-julia">combined &#61; let
    rng &#61; StableRNG&#40;1&#41;
    &#91;i &#37; 2 &#61;&#61; 1 ? rand&#40;rng, d1&#41; : rand&#40;rng, d2&#41; for i in 1:200&#93;
end;</code></pre>
<p>and visualize it:</p>
<p><img src="/assets/latent/combined.png" alt="Combined Sheperd and Terrier data" /></p>
<p>Next to LPA, this problem is also known as <em>gaussian &#40;finite&#41; mixture modelling</em>. When the observed variables are discrete, the appropriate model is known as <em>latent class analysis</em> &#40;LCA&#41; or <em>binomial &#40;finite&#41; mixture model</em>. LCA is also known as <em>latent Dirichlet allocation</em> in the machine learning literature.</p>
<p>As a sidenote, Latent Dirichlet allocation is one of the older methods used for natural language processing tasks &#40;Blei et al., 2003&#41;. By old, here, I mean that it is one of the methods used before the advent of deep learning around 2013. A typical natural language processing tasks would be to classify documents. In this setting, LDA can be used to interpret text by finding words with a similar &quot;topic&quot;. For example, the topic can be &quot;education&quot; with words such as &quot;school&quot;, &quot;students&quot;, &quot;schools&quot; and &quot;teachers&quot; &#40;Blei et al., 2003&#41;. This example also shows one of the main differences between LDA usage in machine learning and LCA usage in science. Both are essentially the same models, but where social science often sticks to a few latent classes to manually interpret, machine learning happily runs the model for 100 latent classes &#40;Blei et al., 2003; p. 1007&#41;.</p>
<h2 id="first_attempt"><a href="#first_attempt" class="header-anchor">First attempt</a></h2>
<p>Enough background, let&#39;s fit a latent profile model via Turing. Based on the <a href="https://turing.ml/dev/tutorials/01-gaussian-mixture-model/">Turing tutorial</a> and a great <a href="https://discourse.julialang.org/t/variational-inference-of-mixture-models/40031/2">Discourse post</a>, we can fit a nice model which runs in less than a minute:</p>
<pre><code class="language-julia">@model function exchangeable_mixture&#40;k::Int, Y&#41;
    w ~ Dirichlet&#40;k, 1&#41;

    μ ~ filldist&#40;Normal&#40;25, 4&#41;, 2&#41;

    n &#61; length&#40;Y&#41;
    Y ~ filldist&#40;MixtureModel&#40;Normal, μ, w&#41;, n&#41;
end;</code></pre>
<pre><code class="language-julia">exchangeable_model &#61; exchangeable_mixture&#40;2, combined&#41;;</code></pre>
<pre><code class="language-julia">n_samples &#61; 1_000;</code></pre>
<p>Just to be sure, let&#39;s sample from the prior to see how things look:</p>
<pre><code class="language-julia">exchangeable_prior &#61; sample&#40;exchangeable_model, Prior&#40;&#41;, n_samples&#41;;</code></pre>
<p><img src="/assets/latent/exchangeable-prior.png" alt="Exchangeable prior" /></p>
<p>This looks as expected. When looking at the observed distribution in the first figure in this blog, it is not unreasonable to set the means for the <code>μ</code> priors around the middle of the data. With that, the sampler has a nice place to start and should be able to estimate the parameters reasonably quickly.</p>
<p>The <code>w</code> specifies the weights of the latent classes. In the <code>combined</code> data, I&#39;ve drawn half of the samples from <code>d1</code> and half of the samples from <code>d2</code>, so the weights should be 0.5 and 0.5. This Dirichlet prior is a, so called, unit simplex. To me it just looks like an uniform prior, but I guess there is a good reason for the unit simplex term. This prior is reasonable because it doesn&#39;t tell the sampler much about the location of the weights except that they are between 0 and 1.</p>
<p>So, the prior look good. It&#39;s time to obtain the posterior. To do this, we need to use HMC and not NUTS. Normally, NUTS is the best sampler to use, but for latent models NUTS is often having problems. Online, people seem to suggest that this is due to the multimodality, that is, because there are two solutions to this problem:</p>
<ol>
<li><p>μ1 estimates the mean of Sheperds and μ2 estimates the mean of Terriers <strong>OR</strong></p>
</li>
<li><p>μ2 estimates the mean of Teriers and μ2 estimates the mean of Shepers.</p>
</li>
</ol>
<p>This is called the <em>identifiability</em> problem &#40;Casella &amp; Berger, 2002&#41;, the <em>label switching problem</em> &#40;Obserki, 2016&#41; or <em>labeling degeneracies</em> &#40;Betancourt, 2017&#41;.</p>
<p>So, let&#39;s sample 3 chains in parallel with <code>HMC</code>:</p>
<pre><code class="language-julia">exchangable_posterior &#61; let
    rng &#61; StableRNG&#40;1&#41;
    sampler &#61; HMC&#40;0.2, 10&#41;
    sample&#40;rng, exchangeable_model, sampler, MCMCThreads&#40;&#41;, n_samples, 3&#41;
end;</code></pre>
<p><img src="/assets/latent/exchangeable-posterior.png" alt="Exchangeable posterior" /></p>
<p>Hmm. That didn&#39;t work. When one or more chains don&#39;t move at all &#40;a horizontal line in the left plot&#41; for mixture models, then try reducing the leapfrog step size &#40;the first argument to <code>HMC</code>&#41;.</p>
<pre><code class="language-julia">exchangable_posterior_smaller_stepsize &#61; let
    rng &#61; StableRNG&#40;1&#41;
    sampler &#61; HMC&#40;0.01, 10&#41;
    sample&#40;rng, exchangeable_model, sampler, MCMCThreads&#40;&#41;, n_samples, 4&#41;
end;</code></pre>
<p><img src="/assets/latent/exchangeable-posterior-smaller-stepsize.png" alt="Exchangeable posterior" /></p>
<p>That looks much better. However, now we&#39;re dealing with the label switching problem. Normally, to get the parameter estimate, we could just take the mean over all the chains. In this case, we couldn&#39;t do that and instead should take the mean over only one chain? That would work, but isn&#39;t ideal either.</p>
<h2 id="fixing_the_label_switching"><a href="#fixing_the_label_switching" class="header-anchor">Fixing the label switching</a></h2>
<p>Betancourt &#40;2017&#41; suggests using an ordered prior for μ. Via <code>Bijectors.jl.OrderedBijector</code> this should be possible in <code>Turing.jl</code> too. Unfortunately, I wasn&#39;t able to figure it out. &#40;It appears that the Stan model is transforming things to the log scale and that works well together with the ordered prior. I&#39;m too lazy to convert things to the log scale and back again, so that&#39;s why I&#39;m not doing that.&#41;</p>
<p>As a workaround, I came up with the idea to enforce the ordering in another way, namely to cutoff the range of possible values via two non-overlapping uniform distributions. This can be thought of as drawing a line through the middle of the two means which will lock both parameters in their own region.</p>
<pre><code class="language-julia">@model function mm&#40;k::Int, Y&#41;
    w ~ Dirichlet&#40;k, 1&#41;
    
    μ1 &#61; Uniform&#40;10, 25&#41;
    μ2 &#61; Uniform&#40;25, 40&#41;
    μ ~ arraydist&#40;&#91;μ1, μ2&#93;&#41;
    
    n &#61; length&#40;Y&#41;
    Y ~ filldist&#40;MixtureModel&#40;Normal, μ, w&#41;, n&#41;
end;</code></pre>
<pre><code class="language-julia">mixture_model &#61; mm&#40;2, combined&#41;;</code></pre>
<pre><code class="language-julia">mixture_model_prior &#61; sample&#40;mixture_model, Prior&#40;&#41;, n_samples&#41;;</code></pre>
<p><img src="/assets/latent/mixture-model-prior.png" alt="Mixture model prior" /></p>
<p>After a bit of fiddling and increasing the number of leapfrog steps to use &#40;the second argument to HMC&#41;, this shows chains with nice convergence and mixing:</p>
<pre><code class="language-julia">mixture_posterior &#61; let
    rng &#61; StableRNG&#40;1&#41;
    sampler &#61; HMC&#40;0.01, 20&#41;
    sample&#40;rng, mixture_model, sampler, MCMCThreads&#40;&#41;, n_samples, 3&#41;
end;</code></pre>
<p><img src="/assets/latent/mixture-posterior.png" alt="Mixture posterior" /></p>
<table><tr><th align="right">parameters</th><th align="right">mean</th><th align="right">std</th><th align="right">mcse</th><th align="right">ess&#95;bulk</th><th align="right">ess&#95;tail</th><th align="right">rhat</th><th align="right">ess&#95;per&#95;sec</th></tr><tr><td align="right"><code>w&#91;1&#93;</code></td><td align="right">0.549693</td><td align="right">0.0843669</td><td align="right">0.0116713</td><td align="right">77.4745</td><td align="right">34.9351</td><td align="right">1.07559</td><td align="right">1.12712</td></tr><tr><td align="right"><code>w&#91;2&#93;</code></td><td align="right">0.450307</td><td align="right">0.0843669</td><td align="right">0.0116713</td><td align="right">77.4745</td><td align="right">34.9351</td><td align="right">1.07559</td><td align="right">1.12712</td></tr><tr><td align="right"><code>μ&#91;1&#93;</code></td><td align="right">22.1324</td><td align="right">1.19495</td><td align="right">0.347121</td><td align="right">24.4974</td><td align="right">18.9747</td><td align="right">1.10219</td><td align="right">0.356393</td></tr><tr><td align="right"><code>μ&#91;2&#93;</code></td><td align="right">28.6765</td><td align="right">1.36629</td><td align="right">0.138542</td><td align="right">148.005</td><td align="right">30.5804</td><td align="right">1.08689</td><td align="right">2.1532</td></tr></table>
<p>And we now have almost correct estimates for all parameter locations. The correct values should be a mean of 22 and a mean of 28, which are almost correctly estimated as can be seen in the mean column.</p>
<h2 id="variational_inference"><a href="#variational_inference" class="header-anchor">Variational inference</a></h2>
<p>Some say that <em>variational inference</em> &#40;VI&#41; can deal much better with mixed models than Markov chain Monte Carlo. &#40;I forgot the reference but read it in some paper while trying to debug models.&#41; Let&#39;s put that claim to the test.</p>
<p>VI doesn&#39;t have such a nice interface as the Monte carlo based models, but we can run multithreaded sampling with only a few lines of code. The outcomes are put in a DataFrame here to allow for easier plotting:</p>
<pre><code class="language-julia">function sample_vi&#40;model; samples_per_step&#61;10, max_iters&#61;1_000&#41;
    n_chains &#61; 3
    dfs &#61; Vector&#123;DataFrame&#125;&#40;undef, n_chains&#41;
    colnames &#61; names&#40;mixture_model_prior, :parameters&#41;
    Threads.@threads for i in 1:n_chains
        q &#61; vi&#40;model, ADVI&#40;samples_per_step, max_iters&#41;&#41;
        M &#61; rand&#40;q, n_samples&#41;::Matrix&#123;Float64&#125;
        df &#61; DataFrame&#40;transpose&#40;M&#41;, colnames&#41;
        df&#91;&#33;, :chain&#93; &#61; fill&#40;i, nrow&#40;df&#41;&#41;
        df&#91;&#33;, :iteration&#93; &#61; 1:nrow&#40;df&#41;
        dfs&#91;i&#93; &#61; df
    end
    vcat&#40;dfs...&#41;
end;</code></pre>
<pre><code class="language-julia">vi_posterior &#61; sample_vi&#40;mixture_model&#41;;</code></pre>
<p><img src="/assets/latent/vi-posterior.png" alt="VI posterior" /></p>
<table><tr><th align="right">parameters</th><th align="right">mean</th><th align="right">std</th></tr><tr><td align="right"><code>w&#91;1&#93;</code></td><td align="right">0.536976</td><td align="right">0.0339081</td></tr><tr><td align="right"><code>w&#91;2&#93;</code></td><td align="right">0.463024</td><td align="right">0.0339081</td></tr><tr><td align="right"><code>μ&#91;1&#93;</code></td><td align="right">21.6626</td><td align="right">0.107995</td></tr><tr><td align="right"><code>μ&#91;2&#93;</code></td><td align="right">28.6174</td><td align="right">0.115107</td></tr></table>
<p>This outcome is</p>
<ul>
<li><p>closer to the correct outcome than some of the Monte Carlo based posteriors,</p>
</li>
<li><p>is easier to get right &#40;less fiddling with sampler parameters&#41; and</p>
</li>
<li><p>runs about 4 times as fast at the time of writing &#40;about 20 seconds for Monte Carlo samplers above versus 5 seconds for VI&#41;.</p>
</li>
</ul>
<p><strong>One caveat though: don&#39;t run only one VI chain on the exchangeable model above&#33;</strong> It will happily give a completely incorrect outcome without showing sampling problems&#33; To avoid that, run multiple VI chains like shown here.</p>
<h2 id="ordered_constraint"><a href="#ordered_constraint" class="header-anchor">Ordered constraint</a></h2>
<p>The drawback of the earlier solutions to the label switching problem is that an estimate should be available for the location of the distributions. This isn&#39;t always the case, especially when the problem would involve more latent distributions than presented here. A better solution would be to enforce an ordering on the means μ, that is, to enforce that μ1 ≤ μ2 ≤ ... ≤ μn for n means. With such an ordering, it is impossible for the labels to switch.</p>
<p>After lots and lots of fiddling, I did manage to use an ordered prior in Turing.jl. Thanks to help by Tor Fjelde in a <a href="https://github.com/TuringLang/Bijectors.jl/issues/209">GitHub issue</a>. The trick is to use the <code>Bijectors.OrderedBijector</code>, put the desired values through the inverse of the bijector and put these outcomes in an <code>ordered&#40;arraydist&#40;...&#41;&#41;</code>.</p>
<p>Also, for technical reasons, the numbers put through the inverse cannot be the same, so that&#39;s why the second number is slightly larger. I&#39;ve fiddled a bit with how much difference there is between the numbers and a smaller difference shows a better prior plot, but worse HMC posterior. Very strange.</p>
<pre><code class="language-julia">inv_ordered&#40;X::Vector&#41; &#61; Bijectors.inverse&#40;Bijectors.OrderedBijector&#40;&#41;&#41;&#40;X&#41;;</code></pre>
<pre><code class="language-julia">M &#61; inv_ordered&#40;&#91;25, 25.01&#93;&#41;</code></pre>
<pre><code class="language-raw">2-element Vector&#123;Float64&#125;:
 25.0
 -4.6051701859879355</code></pre>
<pre><code class="language-julia">@model function ordered_mixture&#40;k::Int, Y&#41;
    w ~ Dirichlet&#40;k, 1&#41;

    μ ~ ordered&#40;arraydist&#40;&#91;Normal&#40;m, 4&#41; for m in M&#93;&#41;&#41;

    n &#61; length&#40;Y&#41;
    Y ~ filldist&#40;MixtureModel&#40;Normal, μ, w&#41;, n&#41;
end;</code></pre>
<pre><code class="language-julia">ordered_model &#61; ordered_mixture&#40;2, combined&#41;;</code></pre>
<p>In the end, the HMC sampler keeps being the most robust. I&#39;ve tried <code>NUTS&#40;1_000, 20&#41;</code> too and it did work albeit taking minutes to finish and giving erratic estimates. Also, I&#39;ve tried VI and that just didn&#39;t work with the ordering constraint and no amount of sampler parameter tuning seemed to solve the problem.</p>
<p>So, there we go, let&#39;s see the <strong>best</strong> model for the data:</p>
<pre><code class="language-julia">ordered_hmc_posterior &#61; let
    rng &#61; StableRNG&#40;1&#41;
    sampler &#61; HMC&#40;0.001, 100&#41;
    sample&#40;rng, ordered_model, sampler, MCMCThreads&#40;&#41;, 2 * n_samples, 3&#41;
end;</code></pre>
<p><img src="/assets/latent/ordered-hmc-posterior.png" alt="Ordered HMC posterior" /></p>
<p>One last thing. Convergence takes a while, so let&#39;s throw away the first 700 samples.</p>
<pre><code class="language-julia">ordered_warmed_posterior &#61; let
    rng &#61; StableRNG&#40;1&#41;
    sampler &#61; HMC&#40;0.001, 100&#41;
    discard_initial &#61; 1000
    sample&#40;rng, ordered_model, sampler, MCMCThreads&#40;&#41;, 2 * n_samples, 3; discard_initial&#41;
end;</code></pre>
<p><img src="/assets/latent/ordered-warmed-posterior.png" alt="Ordered warmed posterior" /></p>
<table><tr><th align="right">parameters</th><th align="right">mean</th><th align="right">std</th><th align="right">mcse</th><th align="right">ess&#95;bulk</th><th align="right">ess&#95;tail</th><th align="right">rhat</th><th align="right">ess&#95;per&#95;sec</th></tr><tr><td align="right"><code>w&#91;1&#93;</code></td><td align="right">0.535497</td><td align="right">0.0356642</td><td align="right">0.00129298</td><td align="right">760.682</td><td align="right">1651.15</td><td align="right">1.00207</td><td align="right">2.94344</td></tr><tr><td align="right"><code>w&#91;2&#93;</code></td><td align="right">0.464503</td><td align="right">0.0356642</td><td align="right">0.00129298</td><td align="right">760.682</td><td align="right">1651.15</td><td align="right">1.00207</td><td align="right">2.94344</td></tr><tr><td align="right"><code>μ&#91;1&#93;</code></td><td align="right">21.6445</td><td align="right">0.109563</td><td align="right">0.00283203</td><td align="right">1497.22</td><td align="right">2632.79</td><td align="right">1.00105</td><td align="right">5.79344</td></tr><tr><td align="right"><code>μ&#91;2&#93;</code></td><td align="right">28.6451</td><td align="right">0.112493</td><td align="right">0.0117881</td><td align="right">91.0354</td><td align="right">245.47</td><td align="right">1.02564</td><td align="right">0.352259</td></tr></table>
<p>Awesome.</p>
<h2 id="conclusion"><a href="#conclusion" class="header-anchor">Conclusion</a></h2>
<p>It is very tricky to get mixture modeling right with Markov chain Monte Carlo. When sampling from the posterior, the sampler couldn&#39;t deal well with the label switching problem. Fiddling like this with Bayesian samplers has benefits too, namely that the problematic sampling did indicate a problem in the model specification.</p>
<p>There were two solutions: One solution is to define very strong location priors so that <code>μ&#91;1&#93;</code> and <code>μ&#91;2&#93;</code> have difficulty switching places. This works with VI and, in turn, reduces the running time and is less sensitive to the choice of sampler parameters. The drawback is that much information is required to know where the location should be or what is a good cutoff point. Another solution is to use the ordered constraint. Unfortunately, I couldn&#39;t get this to work with VI nor with NUTS. Therefore, some manual tuning of sampler parameters is required to get a good outcome. The running time is reasonable with about 20 seconds for the last model in this post.</p>
<p>Overall, it took about six days to write this blog which is a bit more than I would have guessed. The main reason why it took so long was that Turing.jl provides a lot of flexibility when defining the models. In other words, there are many ways in which you can shoot yourself in the foot. At the same time, it&#39;s really great to know that tuning models to specific use-cases is possible unlike frequentist models. Hopefully, this post will provide a helpful foundation for more complex Bayesian mixed models.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<p>Betancourt, M. Identifying Bayesian Mixture Models. Stan documentation. <a href="https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html</a>.</p>
<p>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. &#40;2003&#41;. Latent dirichlet allocation. the Journal of machine Learning research, 3, 993-1022. <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</a>.</p>
<p>Casella, G. &amp; Berger, R. L. &#40;2002&#41;. Statistical Inference. Second edition. Cengage learning.</p>
<p>Gelman, A., Carlin, J. B., Stern, H. S., &amp; Rubin, D. B. &#40;1995&#41;. Bayesian data analysis. Chapman and Hall/CRC. <a href="https://doi.org/10.1201/9780429258411">https://doi.org/10.1201/9780429258411</a>.</p>
<p>Oberski D. L. &#40;2016&#41; Mixture Models: Latent Profile and Latent Class Analysis. In: Robertson J., Kaptein M. &#40;eds&#41; Modern Statistical Methods for HCI. Human–Computer Interaction Series. Springer, Cham. <a href="https://doi.org/10.1007/978-3-319-26633-6_12">https://doi.org/10.1007/978-3-319-26633-6_12</a>.</p>
<h2 id="appendix"><a href="#appendix" class="header-anchor">Appendix</a></h2>
<pre><code class="language-julia">function plot_latent&#40;d1, d2, combined&#41;
	w, h &#61; 1100, 500
	fig &#61; Figure&#40;; size&#61;&#40;w, h&#41;&#41;
	
	ax1 &#61; Axis&#40;fig&#91;1, 1&#93;; title&#61;&quot;Observed distribution&quot;, xlabel&#61;&quot;Weight &#40;kg&#41;&quot;&#41;
	density&#33;&#40;ax1, combined&#41;

	title &#61; &quot;Latent distributions&quot;
	ax2 &#61; Axis&#40;fig&#91;1, 2&#93;; title, xlabel&#61;&quot;Weight &#40;kg&#41;&quot;&#41;
	lower &#61; quantile&#40;d1, 0.001&#41;
	upper &#61; quantile&#40;d2, 0.999&#41;
	I &#61; lower:0.01:upper
	lines&#33;&#40;ax2, I, pdf.&#40;d1, I&#41;; label&#61;&quot;Australian\nShepherd&quot;&#41;
	lines&#33;&#40;ax2, I, pdf.&#40;d2, I&#41;; label&#61;&quot;American\nTerrier&quot;&#41;
	Legend&#40;fig&#91;1, 3&#93;, ax2&#41;
	linkxaxes&#33;&#40;ax1, ax2&#41;
	
	fig
end;</code></pre>
<pre><code class="language-julia">function plot_chains&#40;chns; density_func&#61;density&#33;&#41;
	df &#61; DataFrame&#40;chns&#41;
	n_chains &#61; length&#40;unique&#40;df.chain&#41;&#41;
	n_samples &#61; nrow&#40;df&#41; / n_chains
	df&#91;&#33;, :chain&#93; &#61; string.&#40;df.chain&#41;
	coefs &#61; select&#40;df, :iteration, :chain, r&quot;μ*&quot;, r&quot;w*&quot;&#41;
	cols &#61; filter&#40;n -&gt; startswith&#40;n, r&quot;μ|w&quot;&#41; || n &#61;&#61; &quot;σ&quot;, names&#40;coefs&#41;&#41;
	
	size &#61; &#40;900, 1200&#41;
	fig &#61; Figure&#40;; size&#41;

	values_axs &#61; &#91;Axis&#40;fig&#91;i, 1&#93;; ylabel&#61;string&#40;c&#41;&#41; for &#40;i, c&#41; in enumerate&#40;cols&#41;&#93;
	for &#40;ax, col&#41; in zip&#40;values_axs, cols&#41;
		for i in 1:n_chains
			chain &#61; string&#40;i&#41;
			values &#61; filter&#40;:chain &#61;&gt; &#61;&#61;&#40;chain&#41;, df&#41;&#91;:, col&#93;
			lines&#33;&#40;ax, 1:n_samples, values; label&#61;chain&#41;
		end
	end
	values_axs&#91;end&#93;.xlabel &#61; &quot;Iteration&quot;

	density_axs &#61; &#91;Axis&#40;fig&#91;i, 2&#93;; ylabel&#61;string&#40;c&#41;&#41; for &#40;i, c&#41; in enumerate&#40;cols&#41;&#93;
	for &#40;ax, col&#41; in zip&#40;density_axs, cols&#41;
		for i in 1:n_chains
			chain &#61; string&#40;i&#41;
			values &#61; filter&#40;:chain &#61;&gt; &#61;&#61;&#40;chain&#41;, df&#41;&#91;:, col&#93;
			density_func&#40;ax, values; label&#61;chain&#41;
		end
	end
	density_axs&#91;end&#93;.xlabel &#61; &quot;Parameter estimate&quot;
	w_axs &#61; filter&#40;ax -&gt; startswith&#40;ax.ylabel.val, &quot;w&quot;&#41;, density_axs&#41;
	linkxaxes&#33;&#40;w_axs...&#41;
	μ_axs &#61; filter&#40;ax -&gt; startswith&#40;ax.ylabel.val, &quot;μ&quot;&#41;, density_axs&#41;
	linkxaxes&#33;&#40;μ_axs...&#41;

	return fig
end;</code></pre>
<p>Built with Julia 1.11.3 and</p>
<p>Bijectors 0.14.2 <br />CairoMakie 0.12.16 <br />DataFrames 1.7.0 <br />StableRNGs 1.0.2 <br />Statistics 1.11.1 <br />Turing 0.35.2 </p>
<div class="page-foot">
  <div class="copyright">
    Rik Huijzer.
    The text is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
    The code is licensed under the <a href="https://mit-license.org/">MIT License</a>.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    Last update: 2025-11-01.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>
  const options = {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      // This next line is disabled because it is too much hassle to escape dollars correctly.
      // Pluto.jl doesn't use it since it converts to <span> tags.
      // Franklin.jl also doesn't use it since it converts to `\(` and `\)`.
      // {left: "$", right: "$", display: false},
      {left: "\\begin{equation}", right: "\\end{equation}", display: true},
      {left: "\\begin{align}", right: "\\end{align}", display: true},
      {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
      {left: "\\begin{gather}", right: "\\end{gather}", display: true},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  };

  document.addEventListener('DOMContentLoaded', function() {
    renderMathInElement(document.body, options);
  });
</script>

    
        <script type="text/javascript">
    async function color_ansi() {
        const terminalOutputs = document.querySelectorAll("[id=plutouiterminal]");
        // Avoid loading AnsiUp if there is no terminal output on the page.
        if (terminalOutputs.length == 0) {
            return
        };
        try {
            const { default: AnsiUp } = await import("/libs/ansi_up.js");
            const ansiUp = new AnsiUp();
            // Indexed loop is needed here, the array iterator doesn't work for some reason.
            for (let i = 0; i < terminalOutputs.length; ++i) {
                const terminalOutput = terminalOutputs[i];
                const txt = terminalOutput.innerHTML;
                terminalOutput.innerHTML = ansiUp.ansi_to_html(txt);
            };
        } catch(e) {
            console.error("Failed to import/call ansiup!", e);
        };
    };
    color_ansi();
</script>

    
    
        <!-- http://tutsplus.github.io/clipboard/ -->

<script>
document.addEventListener('DOMContentLoaded', function() {

	// Get the elements.
	// - the 'pre' element.
	// - the 'div' with the 'paste-content' id.

	var pre = document.getElementsByTagName('pre');

	// Add a copy button in the 'pre' element.
	// which only has the className of 'language-' or ' hljs'(if enable highlight.js pre-render).

	for (var i = 0; i < pre.length; i++) {
		var tag_name = pre[i].children[0].className
            	var isLanguage = tag_name.startsWith('language-') || tag_name.endsWith(' hljs');
		if ( isLanguage ) {
			var button           = document.createElement('button');
					button.className = 'copy-button';
					// button.textContent = 'Copy to clipboard';

					pre[i].appendChild(button);
		}
	};

	// Run Clipboard

	var copyCode = new Clipboard('.copy-button', {
		target: function(trigger) {
			return trigger.previousElementSibling;
    }
	});

	// On success:
	// - Change the "Copy" text to "Copied".
	// - Swap it to "Copy" in 2s.
	// - Lead user to the "contenteditable" area with Velocity scroll.

	copyCode.on('success', function(event) {
		event.clearSelection();
		event.trigger.textContent = '    Copied';
		window.setTimeout(function() {
			event.trigger.textContent = '';
		}, 2000);

	});

	// On error (Safari):
	// - Change the  "Press Ctrl+C to copy"
	// - Swap it to "Copy" in 2s.

	copyCode.on('error', function(event) {
		event.trigger.textContent = 'Press "Ctrl + C" to copy';
		window.setTimeout(function() {
			event.trigger.textContent = 'Copy';
		}, 5000);
	});

});
</script>

    
  </body>
</html>
